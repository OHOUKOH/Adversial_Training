{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#  A.I. Seminar Project\n",
    "## Defending Against Adversarial Examples\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fadji OHOUKOH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of adversarial attacks on machine learning models, a defense scheme is a strategy or method used to protect the model from these attacks. The goal of a defense scheme is to make the model robust against adversarial examples, which are input data designed to mislead the model into making incorrect predictions.\n",
    "\n",
    "There are several types of defense schemes, including:\n",
    "\n",
    "1. **Adversarial Training**: This involves including adversarial examples in the training data and retraining the model. The idea is to make the model aware of the kind of perturbations it might face, so it can learn to correctly classify even adversarially perturbed inputs.\n",
    "\n",
    "2. **Defensive Distillation**: This is a process where a second model is trained to mimic the output of the original model but with a softened output distribution. The second model, called the distilled model, is less likely to be affected by small perturbations in the input space, making it more robust against adversarial attacks.\n",
    "\n",
    "3. **Feature Squeezing**: This reduces the search space available to an adversary by coalescing similar inputs into one, thereby limiting the effectiveness of adversarial perturbations.\n",
    "\n",
    "4. **Gradient Masking or Obfuscation**: These methods aim to hide the gradients of the model from the attacker, making it harder to craft effective adversarial examples.\n",
    "\n",
    "5. **Regularization Techniques**: These methods add a penalty term to the loss function during training to encourage the model to learn a simpler (and hopefully more robust) function.\n",
    "\n",
    "Each of these defense schemes has its own strengths and weaknesses, and the effectiveness of a particular scheme can depend on the specific model and threat model. In practice, it's common to use a combination of multiple defense schemes to achieve the best robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: torch.Size([60000, 28, 28])\n",
      "Shape of X_test: torch.Size([10000, 28, 28])\n",
      "Shape of y_train: torch.Size([60000])\n",
      "Shape of y_test: torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a transform to convert images to PyTorch tensors\n",
    "transform = transforms.ToTensor()\n",
    "mnist = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "# Load the MNIST dataset\n",
    "data = mnist.data\n",
    "target = mnist.targets  # Use mnist.targets instead of mnist.target\n",
    "#Defining the training set \n",
    "X_train=data\n",
    "y_train=target\n",
    "#Defining the test set \n",
    "mnist1=datasets.MNIST(root='./data',train=False,transform=transform , download=True)\n",
    "X_test=mnist1.data / 255\n",
    "y_test=mnist1.targets / 255\n",
    "# Print the shapes of the train and test sets\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ModelM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelM, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5, stride=1, bias=False)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, bias=False)\n",
    "        self.fc1 = nn.Linear(64 * 5 * 5, 64, bias=False)\n",
    "        self.fc2 = nn.Linear(64, 512, bias=False)\n",
    "        self.fc3 = nn.Linear(512, 10, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = ModelM()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Useful explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# White Box setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of adversarial attacks on machine learning models, a \"white-box\" setting refers to a scenario where the attacker has complete knowledge of the model. This includes the architecture of the model, the parameters (weights and biases), the training method, and even the specific data points used for training.\n",
    "\n",
    "\n",
    "\n",
    "This is in contrast to a \"black-box\" setting, where the attacker only has access to the inputs to the model and the corresponding outputs, without knowing the details of the model's architecture or parameters.\n",
    "\n",
    "The white-box setting represents a worst-case scenario from a security perspective, as it assumes the attacker has maximum knowledge. Therefore, defenses that are effective in the white-box setting are considered to be very robust. However, it's also a less realistic scenario, as attackers in real-world situations are unlikely to have complete knowledge of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "batch_size = 150\n",
    "X_train_reshaped = X_train.unsqueeze(1).to(torch.float32)\n",
    "y_train = y_train.to(torch.long)\n",
    "\n",
    "# Create a DataLoader for the training set\n",
    "train_dataset = TensorDataset(X_train_reshaped, y_train)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Batch 100/400, Loss: 1.1453036069869995\n",
      "Epoch 1/10, Batch 200/400, Loss: 0.5568670630455017\n",
      "Epoch 1/10, Batch 300/400, Loss: 0.396942675113678\n",
      "Epoch 1/10, Batch 400/400, Loss: 0.4898703992366791\n",
      "Epoch 2/10, Batch 100/400, Loss: 0.3437506854534149\n",
      "Epoch 2/10, Batch 200/400, Loss: 0.3396545946598053\n",
      "Epoch 2/10, Batch 300/400, Loss: 0.39722955226898193\n",
      "Epoch 2/10, Batch 400/400, Loss: 0.478738397359848\n",
      "Epoch 3/10, Batch 100/400, Loss: 0.5156795382499695\n",
      "Epoch 3/10, Batch 200/400, Loss: 0.3717421889305115\n",
      "Epoch 3/10, Batch 300/400, Loss: 0.2956853210926056\n",
      "Epoch 3/10, Batch 400/400, Loss: 0.40921634435653687\n",
      "Epoch 4/10, Batch 100/400, Loss: 0.29066962003707886\n",
      "Epoch 4/10, Batch 200/400, Loss: 0.29348745942115784\n",
      "Epoch 4/10, Batch 300/400, Loss: 0.26768726110458374\n",
      "Epoch 4/10, Batch 400/400, Loss: 0.22766812145709991\n",
      "Epoch 5/10, Batch 100/400, Loss: 0.2814520299434662\n",
      "Epoch 5/10, Batch 200/400, Loss: 0.2984856367111206\n",
      "Epoch 5/10, Batch 300/400, Loss: 0.30899232625961304\n",
      "Epoch 5/10, Batch 400/400, Loss: 0.23541486263275146\n",
      "Epoch 6/10, Batch 100/400, Loss: 0.3334280252456665\n",
      "Epoch 6/10, Batch 200/400, Loss: 0.15966393053531647\n",
      "Epoch 6/10, Batch 300/400, Loss: 0.3532133996486664\n",
      "Epoch 6/10, Batch 400/400, Loss: 0.2977037727832794\n",
      "Epoch 7/10, Batch 100/400, Loss: 0.20920304954051971\n",
      "Epoch 7/10, Batch 200/400, Loss: 0.23590756952762604\n",
      "Epoch 7/10, Batch 300/400, Loss: 0.24335861206054688\n",
      "Epoch 7/10, Batch 400/400, Loss: 0.3605280816555023\n",
      "Epoch 8/10, Batch 100/400, Loss: 0.2815926969051361\n",
      "Epoch 8/10, Batch 200/400, Loss: 0.25849902629852295\n",
      "Epoch 8/10, Batch 300/400, Loss: 0.26410990953445435\n",
      "Epoch 8/10, Batch 400/400, Loss: 0.251543790102005\n",
      "Epoch 9/10, Batch 100/400, Loss: 0.24299973249435425\n",
      "Epoch 9/10, Batch 200/400, Loss: 0.22396820783615112\n",
      "Epoch 9/10, Batch 300/400, Loss: 0.28423288464546204\n",
      "Epoch 9/10, Batch 400/400, Loss: 0.2768113613128662\n",
      "Epoch 10/10, Batch 100/400, Loss: 0.20497560501098633\n",
      "Epoch 10/10, Batch 200/400, Loss: 0.20972208678722382\n",
      "Epoch 10/10, Batch 300/400, Loss: 0.18715685606002808\n",
      "Epoch 10/10, Batch 400/400, Loss: 0.2850281000137329\n",
      "Total training accuracy: 88.41449999999999%\n",
      "Finished Adversarial Training\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "\n",
    "model = ModelM()\n",
    "\n",
    "criterion = CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-2)  # L2 regularization\n",
    "scheduler = StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "epsilon = 0.3\n",
    "alpha = 0.01\n",
    "k = 40\n",
    "Q = [0, 1]  # Assuming pixel values are in the range [0, 1]\n",
    "num_epochs = 10  # Increase the number of epochs\n",
    "best_val_acc = 0  # For early stopping\n",
    "total_predictions =0\n",
    "total_correct=0\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        # Perform the l∞ PGD attack\n",
    "        inputs_adv = inputs.data + epsilon * (2 * torch.rand_like(inputs) - 1)\n",
    "        inputs_adv.requires_grad = True\n",
    "\n",
    "        for _ in range(k):\n",
    "            outputs_adv = model(inputs_adv)\n",
    "            loss_adv = criterion(outputs_adv, labels)\n",
    "            loss_adv.backward()\n",
    "\n",
    "            inputs_adv_grad = alpha * torch.sign(inputs_adv.grad.data)\n",
    "            inputs_adv = inputs_adv.detach() + inputs_adv_grad\n",
    "            inputs_adv = torch.min(torch.max(inputs_adv, inputs - epsilon), inputs + epsilon)\n",
    "            inputs_adv = torch.clamp(inputs_adv, Q[0], Q[1])  # Clip to valid pixel range\n",
    "            inputs_adv.requires_grad = True\n",
    "\n",
    "        # Update the model\n",
    "        outputs = model(inputs_adv)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_predictions += labels.size(0)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Batch {i+1}/{len(train_loader)}, Loss: {loss.item()}\")\n",
    "\n",
    "    # Decay learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    # Early stopping\n",
    "    #val_acc = evaluate(model, data_loader)  # You need to implement the evaluate function\n",
    "    #if val_acc > best_val_acc:\n",
    "    #    best_val_acc = val_acc\n",
    "    #else:\n",
    "    #    print(\"Early stopping\")\n",
    "    #    break\n",
    "\n",
    "# Calculate and print the accuracy for the total training set\n",
    "total_accuracy = total_correct / total_predictions * 100\n",
    "print(f\"Total training accuracy: {total_accuracy}%\")\n",
    "\n",
    "print('Finished Adversarial Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total test accuracy: 10.01%\n"
     ]
    }
   ],
   "source": [
    "# Assuming X_test is your input test data and y_test are your test labels\n",
    "X_test_reshaped = X_test.unsqueeze(1).to(torch.float32)\n",
    "y_test = y_test.to(torch.long)\n",
    "\n",
    "# Create a DataLoader for the test set\n",
    "test_dataset = TensorDataset(X_test_reshaped, y_test)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Switch the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# No need to track gradients for test data, so we use torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        # Perform the l∞ PGD attack\n",
    "        inputs_adv = inputs.data + epsilon * (2 * torch.rand_like(inputs) - 1)\n",
    "        #inputs_adv.requires_grad = True\n",
    "\n",
    "        for _ in range(k):\n",
    "            outputs_adv = model(inputs_adv)\n",
    "            loss_adv = criterion(outputs_adv, labels)\n",
    "            #loss_adv.backward()\n",
    "\n",
    "            inputs_adv_grad = alpha * torch.sign(inputs_adv)\n",
    "            inputs_adv = inputs_adv.detach() + inputs_adv_grad\n",
    "            inputs_adv = torch.min(torch.max(inputs_adv, inputs - epsilon), inputs + epsilon)\n",
    "            inputs_adv = torch.clamp(inputs_adv, Q[0], Q[1])  # Clip to valid pixel range\n",
    "            inputs_adv.requires_grad = True\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs_adv)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_predictions += labels.size(0)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "\n",
    "# Calculate and print the accuracy for the total test set\n",
    "total_accuracy = total_correct / total_predictions * 100\n",
    "print(f\"Total test accuracy: {total_accuracy}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on clean test images: 9 %\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():  # No need to calculate gradients\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the model on clean test images: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model's state dictionary\n",
    "torch.save(model.state_dict(), \"D:\\Master_2022-2024\\M2\\AI_SEMINAR\\Project\\Model_PGD.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on clean test images: 9 %\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():  # No need to calculate gradients\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the model on clean test images: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model's state dictionary\n",
    "torch.save(model.state_dict(), \"D:\\Master_2022-2024\\M2\\AI_SEMINAR\\Project\\Model_PGD.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversial testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X_test is your input test data and y_test are your test labels\n",
    "X_test_reshaped = X_test.unsqueeze(1).to(torch.float32)\n",
    "y_test = y_test.to(torch.long)\n",
    "\n",
    "# Create a DataLoader for the test set\n",
    "test_dataset = TensorDataset(X_test_reshaped, y_test)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Switch the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "total_correct = 0\n",
    "total_predictions = 0\n",
    "\n",
    "# No need to track gradients for test data, so we use torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        # Perform the l∞ PGD attack\n",
    "        inputs_adv = inputs.data + epsilon * (2 * torch.rand_like(inputs) - 1)\n",
    "        #inputs_adv.requires_grad = True\n",
    "\n",
    "        for _ in range(k):\n",
    "            outputs_adv = model(inputs_adv)\n",
    "            loss_adv = criterion(outputs_adv, labels)\n",
    "            #loss_adv.backward()\n",
    "\n",
    "            inputs_adv_grad = alpha * torch.sign(inputs_adv)\n",
    "            inputs_adv = inputs_adv.detach() + inputs_adv_grad\n",
    "            inputs_adv = torch.min(torch.max(inputs_adv, inputs - epsilon), inputs + epsilon)\n",
    "            inputs_adv = torch.clamp(inputs_adv, Q[0], Q[1])  # Clip to valid pixel range\n",
    "            inputs_adv.requires_grad = True\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs_adv)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_predictions += labels.size(0)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "\n",
    "# Calculate and print the accuracy for the total test set\n",
    "total_accuracy = total_correct / total_predictions * 100\n",
    "print(f\"Total test accuracy: {total_accuracy}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal Testing with the MNIST test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():  # No need to calculate gradients\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the model on clean test images: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another way optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_reshaped = X_train.unsqueeze(1).to(torch.float32)\n",
    "y_train = y_train.to(torch.long)\n",
    "train = TensorDataset(x_train_reshaped, y_train)\n",
    "train_loader = DataLoader(train, batch_size=128, shuffle=True)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "total_prediction = 0\n",
    "total_predicted = 0\n",
    "Loss = []\n",
    "num_epochs = 10\n",
    "\n",
    "# Wrapping the model with Foolbox's PyTorchModel\n",
    "preprocessing = dict(mean=[0.1307], std=[0.3081], axis=-3)\n",
    "fmodel = PyTorchModel(model, bounds=(0, 1), preprocessing=preprocessing)\n",
    "\n",
    "# ADAM optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# The linfinity PGD attack\n",
    "attack = LinfPGD()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # Set the model to evaluation mode before the attack\n",
    "        model.eval()\n",
    "        data = data/ data.max().item()\n",
    "        # Generate adversarial examples\n",
    "        _, advs, success = attack(fmodel, data, target, epsilons=[0.3])\n",
    "        #print(type(advs))\n",
    "        # Normalize advs by dividing each element by the maximum value\n",
    "        max_advs = max(advs, key=lambda x: x.max().item())\n",
    "        normalized_advs = [adv / max_advs.max().item() for adv in advs]\n",
    "        # Set the model back to training mode after the attack\n",
    "        model.train()\n",
    "\n",
    "        # Print information about input bounds and adversarial example range\n",
    "        #print(f\"Model Bounds: {fmodel.bounds}\")\n",
    "        #print(f\"Adversarial Examples Range: [{advs.min().item()}, {advs.max().item()}]\")\n",
    "\n",
    "        # Verify that adversarial examples are within the model's input bounds\n",
    "        #assert advs.min().item() >= 0 and advs.max().item() <= 1\n",
    "        advs = torch.stack(advs)\n",
    "        #print(advs.size())\n",
    "        advs = advs.squeeze(0)\n",
    "        #print(advs.size())\n",
    "        # Use the adversarial examples for training\n",
    "        optimizer.zero_grad()\n",
    "        output = model(advs)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, predicted = torch.max(output, axis=1)\n",
    "        total_predicted += (predicted == target).sum().item()\n",
    "        total_prediction += target.size(0)\n",
    "        if (batch % 100) == 0:\n",
    "            Loss.append(loss.item())\n",
    "\n",
    "perturbed_accuracy = total_predicted / total_prediction * 100\n",
    "print(f'Training accuracy: {perturbed_accuracy}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Black-box setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SPSA (Simultaneous Perturbation Stochastic Approximation) attack is a black-box adversarial attack that doesn't require access to the model's gradients. Instead, it estimates the gradient using random perturbations. This makes it effective against models that use gradient masking or obfuscation as a defense.\n",
    "\n",
    "We can  implement these attacks using Foolbox by  using the `foolbox.attacks.SPSA` class for the SPSA attack, and the `foolbox.attacks.TransferAttack` class for the transfer attack. Here's an example:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " A high accuracy under the SPSA attack means that the model is robust against this specific type of attack. It doesn't directly indicate whether the model is exhibiting gradient masking.\n",
    "\n",
    "Gradient masking refers to the phenomenon where the gradients of a model do not provide useful information for crafting adversarial examples. This can make the model appear robust against gradient-based attacks, while it might still be vulnerable to other types of attacks, especially those that do not rely on gradients, like the SPSA attack.\n",
    "\n",
    "If a model is robust against both gradient-based attacks and non-gradient-based attacks like SPSA, it's a good sign that the model is genuinely robust, not just exhibiting gradient masking. \n",
    "\n",
    "However, if a model appears robust against gradient-based attacks but is vulnerable to the SPSA attack, it could be a sign of gradient masking. In this case, the model's apparent robustness against gradient-based attacks might be due to the gradients being uninformative, not because the model is genuinely robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def spsa_attack(f, x0, delta, alpha, n, epsilon, T):\n",
    "    D = x0.numel()\n",
    "    x = x0.clone().detach().requires_grad_(True)\n",
    "\n",
    "    for t in range(T):\n",
    "        # Sample v\n",
    "        v = torch.randint(0, 2, size=(n, D)) * 2 - 1  # {1, -1}^D\n",
    "        v = v.to(x0.device)\n",
    "\n",
    "        # Calculate g\n",
    "        x_plus_delta_v = x + delta * v\n",
    "        x_minus_delta_v = x - delta * v\n",
    "        g = (f(x_plus_delta_v) - f(x_minus_delta_v)) * v / (2 * delta)\n",
    "\n",
    "        # Update x\n",
    "        x_prime = x - alpha * g.mean(dim=0)\n",
    "        \n",
    "        # Project x\n",
    "        diff = x_prime - x0\n",
    "        diff = diff / diff.norm() * min(diff.norm(), epsilon)\n",
    "        x = x0 + diff\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer attacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer attacks involve generating adversarial examples using one model (the source model), and then testing them on another model (the target model). This simulates a black-box attack scenario where the attacker doesn't have direct access to the target model's architecture or parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from foolbox import PyTorchModel, accuracy, samples\n",
    "from foolbox.attacks import LinfPGD\n",
    "\n",
    "\n",
    "# source model\n",
    "class SourceModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SourceModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, bias=False)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, bias=False)\n",
    "        self.fc1 = nn.Linear(64 * 6 * 6, 128, bias=False)\n",
    "        self.fc2 = nn.Linear(128, 512, bias=False)\n",
    "        self.fc3 = nn.Linear(512, 10, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 6 * 6)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "source_model = SourceModel()\n",
    "\n",
    "# Create Foolbox models\n",
    "fmodel_source = PyTorchModel(source_model, bounds=(0, 1))\n",
    "fmodel_target = PyTorchModel(model, bounds=(0, 1))\n",
    "\n",
    "# Generate adversarial examples using the source model\n",
    "attack = LinfPGD()\n",
    "epsilons = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "_, advs_transfer, success_transfer = attack(fmodel_source, images, labels, epsilons=epsilons)\n",
    "\n",
    "# Calculate and print the accuracy of the defended model on the adversarial examples\n",
    "print('Accuracy on SPSA adversarial examples:', accuracy(fmodel_target, advs_spsa, labels))\n",
    "print('Accuracy on transfer adversarial examples:', accuracy(fmodel_target, advs_transfer, labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
